# Build BioMistral Medical RAG Chatbot using BioMistral Open Source LLM 
# ü©∫ Medical Chatbot using RAG (Retrieval-Augmented Generation)

This project is a Medical Chatbot designed using Retrieval-Augmented Generation (RAG) architecture. The chatbot utilizes advanced machine learning models, including large language models (LLMs) and embeddings, to provide accurate and relevant responses based on the provided context. The chatbot processes medical documents and allows users to ask health-related questions. 

<img width="25%" src="https://img.freepik.com/free-vector/chatbot-chat-message-vectorart_78370-4104.jpg?semt=ais_hybrid">

---

### üìÑ Data Source: 
**HealthyHeart.pdf**  
[Healthy Heart PDF](https://www.nhlbi.nih.gov/files/docs/public/heart/healthyheart.pdf)

---

## üöÄ Frameworks and Technologies Used:

- **Langchain**: <u>Pipeline</u> management and framework integration.
- **Llama**: <u>Large Language Model (LLM)</u> used for natural language understanding.
- **Sentence-Transformers**: <u>Embedding model</u> to generate dense representations for each document chunk.
- **Chroma**: <u>Vector store</u> for storing and retrieving document embeddings efficiently.

---

## ü§ñ LLM Model:
**BioMistral-7B**  
[BioMistral-7B-GGUF Model](https://huggingface.co/MazivarPanahi/BioMistral-7B-GGUF/tree/main)

---

## üß©üß¨ Embeddings Model:
**PubMedBert-Base-embeddings**  
[PubMedBert Embeddings](https://huggingface.co/NeuML/pubmedbert-base-embeddings)

---

## üåê Process
1. Load the document and parse the 
2. Divide text into chunks - chunking
3. Create embedding vectors for each chunk
4. Store chunks and embeddings to Vector Store
5. Load LLM model
6. Build application chain end to end
7. Query the chatbot
 -Pass query to Retriever
 -Retrieves relevant docs from Vector Store (KNN)
 -Pass both query and docs to LLM
 -Generate the response

---

##  Process Overview:

1. **Load the Document**: The chatbot loads and parses the `HealthyHeart.pdf` document.
2. **Chunking**: The document is split into smaller, manageable chunks.
3. **Embedding Creation**: Each chunk is transformed into dense embedding vectors using Sentence-Transformers.
4. **Vector Store**: Chunks and their embeddings are stored in Chroma Vector Store for efficient retrieval.
5. **LLM Model Loading**: The BioMistral-7B model is loaded to handle natural language understanding.
6. **Application Chain**: The full application chain is built end-to-end.
7. **Query the Chatbot**: Users can input queries to interact with the chatbot.
8. **Retriever**: The chatbot retrieves relevant document chunks from the Vector Store using K-Nearest Neighbors (KNN) search.
9. **LLM Interaction**: Both the query and relevant documents are passed to the LLM.
10. **Response Generation**: The chatbot generates responses based on the query and document context.

---

## üìö Resources:

- [Langchain Documentation](https://langchain.com/docs)
- [BioMistral-7B Model](https://huggingface.co/MazivarPanahi/BioMistral-7B-GGUF/tree/main)
- [PubMedBert Embeddings](https://huggingface.co/NeuML/pubmedbert-base-embeddings)
